# generate_image.py
# ----------------------------------------
# CPUã‚µãƒ¼ãƒãƒ¼ä¸Šã§ ver1 LoRA ã‚’ä½¿ã£ã¦ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# - ãƒ™ãƒ¼ã‚¹    : runwayml/stable-diffusion-v1-5
# - LoRA     : models/lora_ver1_fp16.safetensors
# - è§£åƒåº¦   : å®Œå…¨å›ºå®š 16:9ï¼ˆ912 x 512ï¼‰
# - CPUå‘ã‘æœ€é©åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ãƒ»é€Ÿåº¦æ”¹å–„ï¼‰
# - LoRA ãŒç¢ºå®Ÿã«é©ç”¨ã•ã‚Œã‚‹æœ€æ–°ç‰ˆã‚³ãƒ¼ãƒ‰
# ----------------------------------------

import argparse
from pathlib import Path
import os
import traceback

import torch
from diffusers import StableDiffusionPipeline


# -----------------------------------------------------
# Stable Diffusion v1.5 + LoRA(ver1) ã‚’èª­ã¿è¾¼ã‚€
# -----------------------------------------------------
def load_pipeline(base_model_id: str, lora_path: Path, device: str = "cpu") -> StableDiffusionPipeline:
    print("[INFO] Base model ã‚’èª­ã¿è¾¼ã¿ä¸­:", base_model_id)

    pipe = StableDiffusionPipeline.from_pretrained(
        base_model_id,
        torch_dtype=torch.float32,  # CPU ã¯ fp32 å›ºå®š
        safety_checker=None,
    )

    # CPUå‘ã‘ãƒ¡ãƒ¢ãƒªç¯€ç´„
    if hasattr(pipe, "enable_attention_slicing"):
        pipe.enable_attention_slicing()
        print("[INFO] attention_slicing ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")

    if hasattr(pipe, "enable_vae_slicing"):
        pipe.enable_vae_slicing()
        print("[INFO] vae_slicing ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")

    # -------------------------------------------------
    # ğŸ”¥ LoRA èª­ã¿è¾¼ã¿ï¼ˆæœ€æ–°ç‰ˆã®å®‰å®šå‹•ä½œï¼‰
    # -------------------------------------------------
    if lora_path.is_file():
        print("[INFO] LoRA ã‚’èª­ã¿è¾¼ã¿ä¸­:", lora_path)

        pipe.load_lora_weights(str(lora_path))

        # diffusers â‰¥ 0.24 ã¯ fuse_lora ãŒå¿…è¦
        if hasattr(pipe, "fuse_lora"):
            print("[INFO] fuse_lora ã‚’å®Ÿè¡Œï¼ˆLoRA ã‚’ãƒ¢ãƒ‡ãƒ«ã«çµ±åˆï¼‰")
            pipe.fuse_lora()
            print("[INFO] LoRA çµ±åˆå®Œäº†ï¼ˆver1 æœ‰åŠ¹åŒ–ï¼‰")

    else:
        print("[WARN] LoRA(ver1) ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€ç´ ã®SD1.5ã§ç”Ÿæˆã—ã¾ã™")

    pipe.to(device)
    return pipe


# -----------------------------------------------------
# ç”»åƒç”Ÿæˆï¼ˆå¸¸ã« 16:9 = 912 Ã— 512ï¼‰
# -----------------------------------------------------
def generate_image(
    prompt: str,
    negative_prompt: str,
    output_path: Path,
    seed: int = 42,
    num_inference_steps: int = 30,
    guidance_scale: float = 7.5,
    device: str = "cpu",
) -> None:

    # â˜… 16:9 å®Œå…¨å›ºå®š
    width = 912
    height = 512

    project_root = Path(__file__).resolve().parent
    lora_path = project_root / "models" / "lora_ver1_fp16.safetensors"

    pipe = load_pipeline("runwayml/stable-diffusion-v1-5", lora_path, device=device)

    generator = torch.Generator(device=device).manual_seed(seed)

    os.makedirs(output_path.parent, exist_ok=True)

    print("[INFO] ç”»åƒç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...")
    print(f"       è§£åƒåº¦: {width}x{height} (16:9å›ºå®š)")
    print(f"       steps: {num_inference_steps}")
    print(f"       guidance: {guidance_scale}")
    print(f"       LoRA: lora_ver1_fp16.safetensors")

    with torch.inference_mode():
        result = pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            width=width,
            height=height,
            generator=generator,
        )

    image = result.images[0]
    image.save(str(output_path))

    print("[INFO] ä¿å­˜ã—ã¾ã—ãŸ:", output_path)


# -----------------------------------------------------
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# -----------------------------------------------------
def main() -> None:
    parser = argparse.ArgumentParser(description="16:9 å›ºå®š ver1 LoRA ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆCPUã‚µãƒ¼ãƒãƒ¼ç”¨ï¼‰")

    parser.add_argument("--prompt", required=False, help="ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæœªæŒ‡å®šãªã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨ï¼‰")

    parser.add_argument(
        "--negative_prompt",
        default=(
            "low quality, bad anatomy, blurry, watermark, text artifact, "
            "photo, realistic photo, 3d render, noisy background, distorted figure"
        ),
        help="ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
    )

    parser.add_argument("--output", default="./output/ver1_sample.png")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--steps", type=int, default=30)
    parser.add_argument("--guidance", type=float, default=7.5)
    parser.add_argument("--device", type=str, default="cpu", choices=["cpu"])

    args = parser.parse_args()

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœªæŒ‡å®š â†’ ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”¨ã®æ±ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    if args.prompt is None:
        args.prompt = (
            "eye-catching blog thumbnail, clean flat illustration, pastel colors, "
            "Japanese blog style, layout-friendly, simple vector design"
        )

    output_path = Path(args.output).resolve()

    print("[INFO] generate_image.py ã‚’é–‹å§‹")
    print("[INFO] ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹:", args.device)

    generate_image(
        prompt=args.prompt,
        negative_prompt=args.negative_prompt,
        output_path=output_path,
        seed=args.seed,
        num_inference_steps=args.steps,
        guidance_scale=args.guidance,
        device=args.device,
    )

    print("[INFO] æ­£å¸¸çµ‚äº†ã—ã¾ã—ãŸ")


# -----------------------------------------------------
if __name__ == "__main__":
    try:
        main()
    except RuntimeError as e:
        msg = str(e)
        print("[ERROR] RuntimeError:", repr(e))
        if "not enough memory" in msg or "DefaultCPUAllocator" in msg:
            print("[HINT] CPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§:")
            print("  - steps ã‚’ 20 ã‹ 15 ã«ä¸‹ã’ã‚‹")
            print("  - VPS ã®RAMãƒ—ãƒ©ãƒ³ã‚’å¢—ã‚„ã™")
        traceback.print_exc()
    except Exception as e:
        print("[ERROR] äºˆæœŸã—ãªã„ä¾‹å¤–:", repr(e))
        traceback.print_exc()
