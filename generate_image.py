# generate_image.py
# ----------------------------------------
# CPUã‚µãƒ¼ãƒãƒ¼ä¸Šã§ ver1 LoRA ã‚’ä½¿ã£ã¦ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# - ãƒ™ãƒ¼ã‚¹    : runwayml/stable-diffusion-v1-5
# - LoRA     : models/lora_ver1_fp16.safetensors
# - è§£åƒåº¦   : å®Œå…¨å›ºå®š 16:9ï¼ˆ912 x 512ï¼‰
# - CPUå‘ã‘æœ€é©åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ãƒ»é€Ÿåº¦æ”¹å–„ï¼‰
# - LoRA ãŒç¢ºå®Ÿã«é©ç”¨ã•ã‚Œã‚‹ã‚ˆã†ã«äºŒæ®µæ§‹ãˆã§èª­ã¿è¾¼ã¿
# ----------------------------------------

import argparse
from pathlib import Path
import os
import traceback

import torch
from diffusers import StableDiffusionPipeline
from diffusers.models.attention_processor import LoRAAttnProcessor


# -----------------------------------------------------
# Stable Diffusion v1.5 + LoRA(ver1) ã‚’èª­ã¿è¾¼ã‚€
# -----------------------------------------------------
def load_pipeline(base_model_id: str, lora_path: Path, device: str = "cpu") -> StableDiffusionPipeline:
    print("[INFO] Base model ã‚’èª­ã¿è¾¼ã¿ä¸­:", base_model_id)

    pipe = StableDiffusionPipeline.from_pretrained(
        base_model_id,
        torch_dtype=torch.float32,  # CPU ã¯ fp32 å›ºå®š
        safety_checker=None,
    )

    # CPUå‘ã‘ãƒ¡ãƒ¢ãƒªç¯€ç´„
    if hasattr(pipe, "enable_attention_slicing"):
        pipe.enable_attention_slicing()
        print("[INFO] attention_slicing ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")

    if hasattr(pipe, "enable_vae_slicing"):
        pipe.enable_vae_slicing()
        print("[INFO] vae_slicing ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")

    # -------------------------------------------------
    # ğŸ”¥ LoRA èª­ã¿è¾¼ã¿ï¼ˆattn_procs / adapter ã®ä¸¡æ–¹ã‚’è©¦ã™ï¼‰
    # -------------------------------------------------
    has_lora = False

    if lora_path.is_file():
        print("[INFO] LoRA ã‚’èª­ã¿è¾¼ã¿ä¸­:", lora_path)

        # â‘  diffusers ã® attn_procs å½¢å¼ã‚’è©¦ã™
        try:
            pipe.unet.load_attn_procs(str(lora_path))
            has_lora = any(
                isinstance(p, LoRAAttnProcessor)
                for p in pipe.unet.attn_processors.values()
            )
            print(f"[DEBUG] load_attn_procs å¾Œ Has LoRAAttnProcessor?: {has_lora}")
        except Exception as e:
            print("[WARN] unet.load_attn_procs ã«å¤±æ•—:", repr(e))

        # â‘¡ ã¾ã åˆºã•ã£ã¦ã„ãªã„å ´åˆã¯ adapter å½¢å¼ã‚’è©¦ã™
        if not has_lora:
            try:
                pipe.load_lora_weights(str(lora_path))
                has_lora = any(
                    isinstance(p, LoRAAttnProcessor)
                    for p in pipe.unet.attn_processors.values()
                )
                print(f"[DEBUG] load_lora_weights å¾Œ Has LoRAAttnProcessor?: {has_lora}")
            except Exception as e:
                print("[WARN] pipe.load_lora_weights ã«å¤±æ•—:", repr(e))

        if has_lora:
            # diffusers â‰¥ 0.24 ç³»ãªã‚‰ fuse_lora ã§çµ±åˆå¯èƒ½
            if hasattr(pipe, "fuse_lora"):
                try:
                    print("[INFO] fuse_lora ã‚’å®Ÿè¡Œï¼ˆLoRA ã‚’ãƒ¢ãƒ‡ãƒ«ã«çµ±åˆï¼‰")
                    pipe.fuse_lora()
                    print("[INFO] LoRA çµ±åˆå®Œäº†ï¼ˆver1 æœ‰åŠ¹åŒ–ï¼‰")
                except Exception as e:
                    # fuse ã«å¤±æ•—ã—ã¦ã‚‚ã€å‹•çš„ LoRA ã¨ã—ã¦ã¯åŠ¹ã„ã¦ã„ã‚‹ã®ã§è‡´å‘½å‚·ã§ã¯ãªã„
                    print("[WARN] fuse_lora ã¯å¤±æ•—ã—ã¾ã—ãŸãŒã€LoRA è‡ªä½“ã¯é©ç”¨æ¸ˆã¿ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™:", repr(e))
        else:
            print("[WARN] LoRA(ver1) ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸãŒã€LoRAAttnProcessor ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            print("       â†’ ver1 ã®å­¦ç¿’çµæœãŒåŠ¹ã„ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    else:
        print("[WARN] LoRA(ver1) ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€ç´ ã®SD1.5ã§ç”Ÿæˆã—ã¾ã™")

    # æœ€çµ‚çš„ãªç¢ºèª
    final_has_lora = any(
        isinstance(p, LoRAAttnProcessor)
        for p in pipe.unet.attn_processors.values()
    )
    print(f"[CHECK] æœ€çµ‚çš„ãª Has LoRAAttnProcessor?: {final_has_lora}")

    pipe.to(device)
    return pipe


# -----------------------------------------------------
# ç”»åƒç”Ÿæˆï¼ˆå¸¸ã« 16:9 = 912 Ã— 512ï¼‰
# -----------------------------------------------------
def generate_image(
    prompt: str,
    negative_prompt: str,
    output_path: Path,
    seed: int = 42,
    num_inference_steps: int = 24,   # â† CPUãªã®ã§ 30 â†’ 24 ã«å°‘ã—ã ã‘çŸ­ç¸®ï¼ˆå¿…è¦ãªã‚‰ 20ã€œ15 ã¾ã§ä¸‹ã’ã¦ã‚‚OKï¼‰
    guidance_scale: float = 7.0,     # å°‘ã—ã ã‘ä¸‹ã’ã¦åæŸã‚’æ—©ã‚ã‚‹
    device: str = "cpu",
) -> None:

    # â˜… 16:9 å®Œå…¨å›ºå®š
    width = 912
    height = 512

    project_root = Path(__file__).resolve().parent
    lora_path = project_root / "models" / "lora_ver1_fp16.safetensors"

    pipe = load_pipeline("runwayml/stable-diffusion-v1-5", lora_path, device=device)

    generator = torch.Generator(device=device).manual_seed(seed)

    os.makedirs(output_path.parent, exist_ok=True)

    print("[INFO] ç”»åƒç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...")
    print(f"       è§£åƒåº¦: {width}x{height} (16:9å›ºå®š)")
    print(f"       steps: {num_inference_steps}")
    print(f"       guidance: {guidance_scale}")
    print(f"       LoRA: lora_ver1_fp16.safetensors")

    # CPU ãªã®ã§ grad ç„¡åŠ¹ & Inference Mode
    torch.set_grad_enabled(False)
    with torch.inference_mode():
        result = pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            width=width,
            height=height,
            generator=generator,
        )

    image = result.images[0]
    image.save(str(output_path))

    print("[INFO] ä¿å­˜ã—ã¾ã—ãŸ:", output_path)


# -----------------------------------------------------
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# -----------------------------------------------------
def main() -> None:
    parser = argparse.ArgumentParser(description="16:9 å›ºå®š ver1 LoRA ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆCPUã‚µãƒ¼ãƒãƒ¼ç”¨ï¼‰")

    parser.add_argument("--prompt", required=False, help="ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæœªæŒ‡å®šãªã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨ï¼‰")

    parser.add_argument(
        "--negative_prompt",
        default=(
            "low quality, bad anatomy, blurry, watermark, text artifact, "
            "photo, realistic photo, 3d render, noisy background, distorted figure"
        ),
        help="ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
    )

    parser.add_argument("--output", default="./output/ver1_sample.png")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--steps", type=int, default=24)   # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚‚ 24 ã«å¯„ã›ã‚‹
    parser.add_argument("--guidance", type=float, default=7.0)
    parser.add_argument("--device", type=str, default="cpu", choices=["cpu"])

    args = parser.parse_args()

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœªæŒ‡å®š â†’ ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”¨ã®æ±ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    if args.prompt is None:
        args.prompt = (
            "eye-catching blog thumbnail, clean flat illustration, pastel colors, "
            "Japanese blog style, layout-friendly, simple vector design"
        )

    output_path = Path(args.output).resolve()

    print("[INFO] generate_image.py ã‚’é–‹å§‹")
    print("[INFO] ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹:", args.device)

    generate_image(
        prompt=args.prompt,
        negative_prompt=args.negative_prompt,
        output_path=output_path,
        seed=args.seed,
        num_inference_steps=args.steps,
        guidance_scale=args.guidance,
        device=args.device,
    )

    print("[INFO] æ­£å¸¸çµ‚äº†ã—ã¾ã—ãŸ")


# -----------------------------------------------------
if __name__ == "__main__":
    try:
        main()
    except RuntimeError as e:
        msg = str(e)
        print("[ERROR] RuntimeError:", repr(e))
        if "not enough memory" in msg or "DefaultCPUAllocator" in msg:
            print("[HINT] CPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§:")
            print("  - steps ã‚’ 20 ã‹ 15 ã«ä¸‹ã’ã‚‹")
            print("  - VPS ã®RAMãƒ—ãƒ©ãƒ³ã‚’å¢—ã‚„ã™")
        traceback.print_exc()
    except Exception as e:
        print("[ERROR] äºˆæœŸã—ãªã„ä¾‹å¤–:", repr(e))
        traceback.print_exc()
